{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\\\\abcd', '\\\\efgh', '{', 'i', 'j', 'k', 'l', 'm', 'n', 'o', '\\\\pqrstuvwxyz', '\\\\}', '\\\\\\\\', '#', 'a', 's', 'd', '#1', '2', '#']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "x = re.compile(r'[^\\\\#]|\\\\[a-zA-Z]+|\\\\[^a-zA-Z]|#[1-9]?')\n",
    "s = r\"\\abcd\\efgh{ijklmno\\pqrstuvwxyz\\}\\\\#asd#12#\"\n",
    "print(x.findall(s))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "outputs": [],
   "source": [
    "from texdocument import TexDocument, TextFragment\n",
    "import abc\n",
    "\n",
    "# import tools for natural language processing\n",
    "import json\n",
    "import math\n",
    "import os\n",
    "import pickle\n",
    "import re\n",
    "from collections import defaultdict\n",
    "from copy import copy\n",
    "from typing import List, Tuple, Optional, Union, Dict\n",
    "\n",
    "import yaml\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import *\n",
    "\n",
    "from parse_text import tokenize_text_nltk, math_env_names\n",
    "\n",
    "\n",
    "expand_tags = {'IN','EX','DT','CC','PDT','WDT','WP','WP$','WRB','IE','EG', 'TO', 'PRP','PRP$'}\n",
    "\n",
    "# statistics class for ngrams\n",
    "# contains following stats:\n",
    "# - ngrams by parts of speech obtained from nltk (number of occurences of each ngram)\n",
    "# - for each i in 1...n each part of ngram(w[1], w[2], w[i-1], w[i+1],..., w[n]) stats of w[i] by parts of speech\n",
    "# - for each word x stats of (w[1], w[2], w[i-1], x, w[i+1],..., w[n])\n",
    "#        for different i for different parts of speech w[1], w[2],..., w[n]\n",
    "class NgramStats:\n",
    "    def __init__(self, n: int):\n",
    "        self.words_stats = None\n",
    "        self.n = n\n",
    "        self.ngrams = defaultdict(lambda: 0)\n",
    "        self.ngrams_by_pos = defaultdict(lambda: defaultdict(lambda: 0))\n",
    "        self.ngrams_by_word = defaultdict(lambda: defaultdict(lambda: 0))\n",
    "        self.words = defaultdict(lambda: 0)\n",
    "\n",
    "    # add ngram to stats\n",
    "    def add_ngram(self, ngram: List[Tuple[str, str]]):  # ngram is a list of tuples (word, part of speech)\n",
    "        parts_of_speech = [ngram[i][1] for i in range(self.n)]  # type: List[Optional[str]]\n",
    "        self.ngrams[tuple(parts_of_speech)] += 1\n",
    "        for i in range(self.n):\n",
    "            parts_of_speech[i] = '_'\n",
    "            self.ngrams_by_pos[tuple(parts_of_speech)][ngram[i][1]] += 1\n",
    "            self.ngrams_by_word[ngram[i][0]][tuple(parts_of_speech)] += 1\n",
    "            parts_of_speech[i] = ngram[i][1]\n",
    "\n",
    "    def add_tokenized_text(self, text: List[List[Tuple[str, str]]]):\n",
    "        for sentence in text:\n",
    "            # replace tags from expand_tags with corresponding words\n",
    "            sentence = [(word, pos) if pos not in expand_tags else (word, '='+word) for word, pos in sentence]\n",
    "            for i in range(len(sentence) - self.n):\n",
    "                self.add_ngram(sentence[i:i + self.n])\n",
    "            for i in range(len(sentence)):\n",
    "                self.words[sentence[i][0]] += 1\n",
    "\n",
    "    def collect_stats_for_words(self, excluded_ps=('EQN', 'EQNP', 'SOL', 'EOL')):\n",
    "        self.words_stats = defaultdict(lambda: [])\n",
    "        for word in self.words:\n",
    "            if word == '' or not word[0].isalpha():\n",
    "                continue\n",
    "            stats = defaultdict(lambda: 0)\n",
    "            total_num = 0\n",
    "            for ngram, num in self.ngrams_by_word[word].items():\n",
    "                total_num += 1\n",
    "                ngram_stats = defaultdict(lambda: 0)\n",
    "                total = 0\n",
    "                for ps, num_ps in self.ngrams_by_pos[ngram].items():\n",
    "                    if ps and ps not in excluded_ps and ps[0].isalpha():\n",
    "                        ngram_stats[ps] += num_ps\n",
    "                        total += num_ps\n",
    "                for ps, num_ps in ngram_stats.items():\n",
    "                    if num_ps > 0:\n",
    "                        stats[ps] += math.log(num_ps)  # ?? do we need to divide by total?\n",
    "            if stats:\n",
    "                for key in stats:\n",
    "                    stats[key] /= total_num\n",
    "                max_stat = max(stats.values())\n",
    "                sub = sum(math.exp(stat - max_stat) for stat in stats.values())\n",
    "                probs = sorted([(key, math.exp(stat - max_stat) / sub) for key, stat in stats.items()],\n",
    "                               reverse=True, key=lambda x: x[1])\n",
    "                self.words_stats[word] = probs\n",
    "\n",
    "        return self.words_stats\n",
    "\n",
    "    def calc_specific_ngrams(self, threshold=2):  # determines ngrams specific for parts of speech\n",
    "        self.ps_ngram_lists = defaultdict(lambda: [])\n",
    "        for key, value in self.ngrams_by_pos.items():\n",
    "            vals = sorted(value.items(), reverse=True, key=lambda x: x[1])\n",
    "            vals = [(x,y) for x,y in vals if x and x[0].isalpha()]\n",
    "            if not vals:\n",
    "                continue\n",
    "            v1 = vals[0][1]\n",
    "            v2 = vals[1][1] if len(vals)>1 else 1\n",
    "            if v1/v2>threshold:\n",
    "                self.ps_ngram_lists[vals[0][0]].append((v1/v2, key))\n",
    "        for key, value in self.ps_ngram_lists:\n",
    "            self.ps_ngram_lists[key].sort(reverse=True)\n",
    "        return self.ps_ngram_lists\n",
    "\n",
    "    def calc_specific(self, word, accept_threshold=(100,10), reject_threshold=(1000,10), excluded_ps=(), include_ps=None):\n",
    "        pos_results = defaultdict(lambda: (0, 0, 0, ()))\n",
    "        rej_results = defaultdict(lambda: (1, 0, 0, ()))\n",
    "        excluded_ps = set(excluded_ps)\n",
    "        excluded_ps.update({'EQN', 'EQNP', 'SOL', 'EOL'})\n",
    "        for ngram, count in self.ngrams_by_word[word].items():\n",
    "            if include_ps:\n",
    "                stats = sorted([(x,y) for x, y in self.ngrams_by_pos[ngram].items() if x and x[0].isalpha() and x in include_ps], reverse=True, key=lambda z: z[1])\n",
    "            else:\n",
    "                stats = sorted([(x,y) for x, y in self.ngrams_by_pos[ngram].items() if x and x[0].isalpha() and x not in excluded_ps], reverse=True, key=lambda z: z[1])\n",
    "            total = sum(y for x,y in stats)\n",
    "            if total < accept_threshold[0] or count<accept_threshold[1]:\n",
    "                continue\n",
    "\n",
    "            mx = stats[0][1]\n",
    "            mx2 = stats[1][1] if len(stats)>1 else 1\n",
    "            pos_results[stats[0][0]] = max(pos_results[stats[0][0]], (mx/mx2, total, count, ngram))\n",
    "            for x, y in stats[1:]:\n",
    "                pos_results[x] = max(pos_results[x], (y/mx, total, count, ngram))\n",
    "                if total >= reject_threshold[0] and count>=reject_threshold[1]:\n",
    "                    rej_results[x] = min(rej_results[x], (y/mx, total, count, ngram))\n",
    "        return pos_results, rej_results\n",
    "\n",
    "\n",
    "\n",
    "    def save_stats(self, path=None):\n",
    "        path = path or f'word_stats_{self.n}.pickle'\n",
    "        with open(path, 'wb') as f:\n",
    "            pickle.dump(self.words_stats, f)\n",
    "\n",
    "    def load_stats(self, path=None):\n",
    "        path = path or f'word_stats_{self.n}.pickle'\n",
    "        with open(path, 'rb') as f:\n",
    "            self.words_stats = pickle.load(f)\n",
    "        return self\n",
    "\n",
    "    def print_stats(self, word, threshold=0.01):\n",
    "        if word not in self.words_stats:\n",
    "            print(f'{word} not found')\n",
    "            return\n",
    "        print(f'{word}:')\n",
    "        ps_width = max(len(ps) for ps in self.words_stats[word])\n",
    "        for ps, prob in self.words_stats[word]:\n",
    "            if prob > threshold:\n",
    "                print(f'{ps:<{ps_width}}: {prob:.3f}')\n",
    "\n",
    "\n",
    "def prepare_tex_file_contents(filename):\n",
    "    document = TexDocument(filename=filename)\n",
    "    # collect text and math environments\n",
    "    text_segments = []\n",
    "    eqn_counter = 0\n",
    "    for env in document.items_and_envs([TextFragment], math_env_names):\n",
    "        if isinstance(env, TextFragment):\n",
    "            text_segments.append(env.remove_formatting_macros())\n",
    "        else:\n",
    "            eqn_counter += 1\n",
    "            text_segments.append(f\"equation_{eqn_counter}\")\n",
    "            last_frag = env.items[-1]\n",
    "            if isinstance(last_frag, TextFragment) and last_frag.text.count('.'):\n",
    "                text_segments.append('.')\n",
    "\n",
    "    return \" \".join(text_segments)\n",
    "\n",
    "\n",
    "# collect ngram stats from file\n",
    "def collect_ngram_stats(n: int, path: str):\n",
    "    stats = NgramStats(n)\n",
    "    if path.endswith('.tex'):\n",
    "        contents = prepare_tex_file_contents(path)\n",
    "    else:\n",
    "        with open(path, 'r') as f:\n",
    "            contents = f.read()\n",
    "    tokenized = tokenize_text_nltk(contents)\n",
    "    print(f'{len(tokenized)} sentences; collecting stats for {n}-grams')\n",
    "    stats.add_tokenized_text(tokenized)\n",
    "    print('collecting stats for words')\n",
    "    stats.collect_stats_for_words()\n",
    "    print('done')\n",
    "    return stats\n",
    "\n",
    "\n",
    "# iterate over files in .tar file\n",
    "import tarfile\n",
    "def iterate_tar_contents(filename):\n",
    "    tar = tarfile.open(filename)\n",
    "    try:\n",
    "        while member := tar.next():\n",
    "            if member.isdir():\n",
    "                continue\n",
    "            content = tar.extractfile(member).read().decode(\"utf-8\")\n",
    "            yield member.name, content\n",
    "    finally:\n",
    "        tar.close()\n",
    "\n",
    "\n",
    "# collect ngram stats from first max_num items of .tar file\n",
    "def collect_ngram_stats_from_tar(n: int, path: str, max_num: int):\n",
    "    stats = NgramStats(n)\n",
    "    print(f'collecting {n}-gram stats from {path}')\n",
    "    for i, (name, content) in enumerate(iterate_tar_contents(path)):\n",
    "        stats.add_tokenized_text(tokenize_text_nltk(content))\n",
    "        if i >= max_num:\n",
    "            break\n",
    "        if (i+1)%100 == 0:\n",
    "            print(f'{i+1} files processed')\n",
    "    print('collecting stats for words')\n",
    "    stats.collect_stats_for_words()\n",
    "    print('done')\n",
    "    return stats\n",
    "\n",
    "\n",
    "def collect_ngram_stats_from_texts(n, texts: Union[list, str], max_num=None, stats_for_words=False):\n",
    "    if isinstance(texts, str):\n",
    "        with open(texts, 'rb') as f:\n",
    "            texts = pickle.load(f)\n",
    "    stats = NgramStats(n)\n",
    "    print(f'collecting {n}-gram stats from {len(texts)} texts')\n",
    "    for i, text in enumerate(texts):\n",
    "        stats.add_tokenized_text(text)\n",
    "        if max_num is not None and i >= max_num:\n",
    "            break\n",
    "        if (i+1)%100 == 0:\n",
    "            print(f'{i+1} texts processed')\n",
    "    if stats_for_words:\n",
    "        print('collecting stats for words')\n",
    "        stats.collect_stats_for_words()\n",
    "    print('done')\n",
    "    return stats\n",
    "\n",
    "\n",
    "def load_tag_texts(path: str, max_num: int):\n",
    "    texts = []\n",
    "    for i, (name, content) in enumerate(iterate_tar_contents(path)):\n",
    "        if i >= max_num:\n",
    "            break\n",
    "        texts.append(tokenize_text_nltk(content))\n",
    "        if (i+1)%100 == 0:\n",
    "            print(f'{i+1} files processed')\n",
    "    return texts\n",
    "\n",
    "def print_pos_rej_results(pos_results, rej_results):\n",
    "    pos_stats = sorted(pos_results.items(), reverse=True, key=lambda x:x[1])\n",
    "    rej_stats = sorted(rej_results.items(), key=lambda x:x[1])\n",
    "    print(\"Positive stats:\")\n",
    "    for key, (value, total, count, ngram) in pos_stats:\n",
    "        print(f\"\\t{key:<5}: {value:10.3f}  | {' '.join(x or '_' for x in ngram):20}  :  {total}  {count}\")\n",
    "\n",
    "    print(\"Negative stats:\")\n",
    "    for key, (value, total, count, ngram) in rej_stats:\n",
    "        print(f\"\\t{key:<5}: {value:10g}  | {' '.join(x or '_' for x in ngram):20}  :  {total}  {count}\")\n",
    "\n",
    "def join_pos_rej(pos, rej):\n",
    "    rpos = pos[0]\n",
    "    rrej = rej[0]\n",
    "    for p in pos[1:]:\n",
    "        for k,v in p.items():\n",
    "            rpos[k] = max(rpos[k], v)\n",
    "    for p in rej[1:]:\n",
    "        for k,v in p.items():\n",
    "            rrej[k] = min(rrej[k], v)\n",
    "    print_pos_rej_results(rpos, rrej)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "collecting 3-gram stats from 1000 texts\n",
      "100 texts processed\n",
      "200 texts processed\n",
      "300 texts processed\n",
      "400 texts processed\n",
      "500 texts processed\n",
      "600 texts processed\n",
      "700 texts processed\n",
      "800 texts processed\n",
      "900 texts processed\n",
      "1000 texts processed\n",
      "collecting stats for words\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "stats = collect_ngram_stats_from_texts(3, texts, max_num=1000)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000 words done\n",
      "2000 words done\n",
      "3000 words done\n",
      "4000 words done\n",
      "5000 words done\n",
      "6000 words done\n"
     ]
    }
   ],
   "source": [
    "save_dict([stats], 'dict_3gram_1000.yml')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "collecting 2-gram stats from 10000 texts\n",
      "100 texts processed\n",
      "200 texts processed\n",
      "300 texts processed\n",
      "400 texts processed\n",
      "500 texts processed\n",
      "600 texts processed\n",
      "700 texts processed\n",
      "800 texts processed\n",
      "900 texts processed\n",
      "1000 texts processed\n",
      "1100 texts processed\n",
      "1200 texts processed\n",
      "1300 texts processed\n",
      "1400 texts processed\n",
      "1500 texts processed\n",
      "1600 texts processed\n",
      "1700 texts processed\n",
      "1800 texts processed\n",
      "1900 texts processed\n",
      "2000 texts processed\n",
      "2100 texts processed\n",
      "2200 texts processed\n",
      "2300 texts processed\n",
      "2400 texts processed\n",
      "2500 texts processed\n",
      "2600 texts processed\n",
      "2700 texts processed\n",
      "2800 texts processed\n",
      "2900 texts processed\n",
      "3000 texts processed\n",
      "3100 texts processed\n",
      "3200 texts processed\n",
      "3300 texts processed\n",
      "3400 texts processed\n",
      "3500 texts processed\n",
      "3600 texts processed\n",
      "3700 texts processed\n",
      "3800 texts processed\n",
      "3900 texts processed\n",
      "4000 texts processed\n",
      "4100 texts processed\n",
      "4200 texts processed\n",
      "4300 texts processed\n",
      "4400 texts processed\n",
      "4500 texts processed\n",
      "4600 texts processed\n",
      "4700 texts processed\n",
      "4800 texts processed\n",
      "4900 texts processed\n",
      "5000 texts processed\n",
      "5100 texts processed\n",
      "5200 texts processed\n",
      "5300 texts processed\n",
      "5400 texts processed\n",
      "5500 texts processed\n",
      "5600 texts processed\n",
      "5700 texts processed\n",
      "5800 texts processed\n",
      "5900 texts processed\n",
      "6000 texts processed\n",
      "6100 texts processed\n",
      "6200 texts processed\n",
      "6300 texts processed\n",
      "6400 texts processed\n",
      "6500 texts processed\n",
      "6600 texts processed\n",
      "6700 texts processed\n",
      "6800 texts processed\n",
      "6900 texts processed\n",
      "7000 texts processed\n",
      "7100 texts processed\n",
      "7200 texts processed\n",
      "7300 texts processed\n",
      "7400 texts processed\n",
      "7500 texts processed\n",
      "7600 texts processed\n",
      "7700 texts processed\n",
      "7800 texts processed\n",
      "7900 texts processed\n",
      "8000 texts processed\n",
      "8100 texts processed\n",
      "8200 texts processed\n",
      "8300 texts processed\n",
      "8400 texts processed\n",
      "8500 texts processed\n",
      "8600 texts processed\n",
      "8700 texts processed\n",
      "8800 texts processed\n",
      "8900 texts processed\n",
      "9000 texts processed\n",
      "9100 texts processed\n",
      "9200 texts processed\n",
      "9300 texts processed\n",
      "9400 texts processed\n",
      "9500 texts processed\n",
      "9600 texts processed\n",
      "9700 texts processed\n",
      "9800 texts processed\n",
      "9900 texts processed\n",
      "10000 texts processed\n",
      "done\n",
      "collecting 3-gram stats from 10000 texts\n",
      "100 texts processed\n",
      "200 texts processed\n",
      "300 texts processed\n",
      "400 texts processed\n",
      "500 texts processed\n",
      "600 texts processed\n",
      "700 texts processed\n",
      "800 texts processed\n",
      "900 texts processed\n",
      "1000 texts processed\n",
      "1100 texts processed\n",
      "1200 texts processed\n",
      "1300 texts processed\n",
      "1400 texts processed\n",
      "1500 texts processed\n",
      "1600 texts processed\n",
      "1700 texts processed\n",
      "1800 texts processed\n",
      "1900 texts processed\n",
      "2000 texts processed\n",
      "2100 texts processed\n",
      "2200 texts processed\n",
      "2300 texts processed\n",
      "2400 texts processed\n",
      "2500 texts processed\n",
      "2600 texts processed\n",
      "2700 texts processed\n",
      "2800 texts processed\n",
      "2900 texts processed\n",
      "3000 texts processed\n",
      "3100 texts processed\n",
      "3200 texts processed\n",
      "3300 texts processed\n",
      "3400 texts processed\n",
      "3500 texts processed\n",
      "3600 texts processed\n",
      "3700 texts processed\n",
      "3800 texts processed\n",
      "3900 texts processed\n",
      "4000 texts processed\n",
      "4100 texts processed\n",
      "4200 texts processed\n",
      "4300 texts processed\n",
      "4400 texts processed\n",
      "4500 texts processed\n",
      "4600 texts processed\n",
      "4700 texts processed\n",
      "4800 texts processed\n",
      "4900 texts processed\n",
      "5000 texts processed\n",
      "5100 texts processed\n",
      "5200 texts processed\n",
      "5300 texts processed\n",
      "5400 texts processed\n",
      "5500 texts processed\n",
      "5600 texts processed\n",
      "5700 texts processed\n",
      "5800 texts processed\n",
      "5900 texts processed\n",
      "6000 texts processed\n",
      "6100 texts processed\n",
      "6200 texts processed\n",
      "6300 texts processed\n",
      "6400 texts processed\n",
      "6500 texts processed\n",
      "6600 texts processed\n",
      "6700 texts processed\n",
      "6800 texts processed\n",
      "6900 texts processed\n",
      "7000 texts processed\n",
      "7100 texts processed\n",
      "7200 texts processed\n",
      "7300 texts processed\n",
      "7400 texts processed\n",
      "7500 texts processed\n",
      "7600 texts processed\n",
      "7700 texts processed\n",
      "7800 texts processed\n",
      "7900 texts processed\n",
      "8000 texts processed\n",
      "8100 texts processed\n",
      "8200 texts processed\n",
      "8300 texts processed\n",
      "8400 texts processed\n",
      "8500 texts processed\n",
      "8600 texts processed\n",
      "8700 texts processed\n",
      "8800 texts processed\n",
      "8900 texts processed\n",
      "9000 texts processed\n",
      "9100 texts processed\n",
      "9200 texts processed\n",
      "9300 texts processed\n",
      "9400 texts processed\n",
      "9500 texts processed\n",
      "9600 texts processed\n",
      "9700 texts processed\n",
      "9800 texts processed\n",
      "9900 texts processed\n",
      "10000 texts processed\n",
      "done\n",
      "collecting 4-gram stats from 10000 texts\n",
      "100 texts processed\n",
      "200 texts processed\n",
      "300 texts processed\n",
      "400 texts processed\n",
      "500 texts processed\n",
      "600 texts processed\n",
      "700 texts processed\n",
      "800 texts processed\n",
      "900 texts processed\n",
      "1000 texts processed\n",
      "1100 texts processed\n",
      "1200 texts processed\n",
      "1300 texts processed\n",
      "1400 texts processed\n",
      "1500 texts processed\n",
      "1600 texts processed\n",
      "1700 texts processed\n",
      "1800 texts processed\n",
      "1900 texts processed\n",
      "2000 texts processed\n",
      "2100 texts processed\n",
      "2200 texts processed\n",
      "2300 texts processed\n",
      "2400 texts processed\n",
      "2500 texts processed\n",
      "2600 texts processed\n",
      "2700 texts processed\n",
      "2800 texts processed\n",
      "2900 texts processed\n",
      "3000 texts processed\n",
      "3100 texts processed\n",
      "3200 texts processed\n",
      "3300 texts processed\n",
      "3400 texts processed\n",
      "3500 texts processed\n",
      "3600 texts processed\n",
      "3700 texts processed\n",
      "3800 texts processed\n",
      "3900 texts processed\n",
      "4000 texts processed\n",
      "4100 texts processed\n",
      "4200 texts processed\n",
      "4300 texts processed\n",
      "4400 texts processed\n",
      "4500 texts processed\n",
      "4600 texts processed\n",
      "4700 texts processed\n",
      "4800 texts processed\n",
      "4900 texts processed\n",
      "5000 texts processed\n",
      "5100 texts processed\n",
      "5200 texts processed\n",
      "5300 texts processed\n",
      "5400 texts processed\n",
      "5500 texts processed\n",
      "5600 texts processed\n",
      "5700 texts processed\n",
      "5800 texts processed\n",
      "5900 texts processed\n",
      "6000 texts processed\n",
      "6100 texts processed\n",
      "6200 texts processed\n",
      "6300 texts processed\n",
      "6400 texts processed\n",
      "6500 texts processed\n",
      "6600 texts processed\n",
      "6700 texts processed\n",
      "6800 texts processed\n",
      "6900 texts processed\n",
      "7000 texts processed\n",
      "7100 texts processed\n",
      "7200 texts processed\n",
      "7300 texts processed\n",
      "7400 texts processed\n",
      "7500 texts processed\n",
      "7600 texts processed\n",
      "7700 texts processed\n",
      "7800 texts processed\n",
      "7900 texts processed\n",
      "8000 texts processed\n",
      "8100 texts processed\n",
      "8200 texts processed\n",
      "8300 texts processed\n",
      "8400 texts processed\n",
      "8500 texts processed\n",
      "8600 texts processed\n",
      "8700 texts processed\n",
      "8800 texts processed\n",
      "8900 texts processed\n",
      "9000 texts processed\n",
      "9100 texts processed\n",
      "9200 texts processed\n",
      "9300 texts processed\n",
      "9400 texts processed\n",
      "9500 texts processed\n",
      "9600 texts processed\n",
      "9700 texts processed\n",
      "9800 texts processed\n",
      "9900 texts processed\n",
      "10000 texts processed\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "import gc\n",
    "stats2=None\n",
    "stats3=None\n",
    "stats4=None\n",
    "gc.collect()\n",
    "stats2 = collect_ngram_stats_from_texts(2, texts, max_num=10000)\n",
    "stats3 = collect_ngram_stats_from_texts(3, texts, max_num=10000)\n",
    "stats4 = collect_ngram_stats_from_texts(4, texts, max_num=10000)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "\n",
    "def save_dict(statss: List[NgramStats], file: str, accept_threshold=(100, 10), reject_threshold=(100, 10), threshold=2, excluded_ps=('NNP', 'NNPS'), filtered_pos=None):\n",
    "    res_dict = {}\n",
    "    with open(file, 'w') as f:\n",
    "        nw = 0\n",
    "        for word in sorted(statss[0].words):\n",
    "            if not word or not word.isalpha() or word[0].isupper():\n",
    "                continue\n",
    "            # if word is stopword, skip it\n",
    "            if word in stop_words:\n",
    "                continue\n",
    "            #print(f'word={word}')\n",
    "            pos = [None]*len(statss)\n",
    "            include_ps = [p for p in filtered_pos[word] if p not in excluded_ps] if filtered_pos and word in filtered_pos else None\n",
    "            for i, stats in enumerate(statss):\n",
    "                pos[i], _ = NgramStats.calc_specific(stats, word, accept_threshold=accept_threshold, reject_threshold=reject_threshold, excluded_ps=excluded_ps, include_ps=include_ps)\n",
    "            rpos = pos[0]\n",
    "            for p in pos[1:]:\n",
    "                for k,v in p.items():\n",
    "                    rpos[k] = max(rpos[k], v)\n",
    "            res = sorted([(v[0],k) for k,v in rpos.items() if v[0]>threshold], reverse=True)\n",
    "            if res:\n",
    "                res_dict[word] = {k: v for v,k in res}\n",
    "                f.write(f'{word}: { {k:v for v,k in res} }\\n')\n",
    "                nw += 1\n",
    "                if nw%1000 == 0:\n",
    "                    print(f'{nw} words done')\n",
    "                    f.flush()\n",
    "    return res_dict"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000 words done\n",
      "2000 words done\n",
      "3000 words done\n",
      "4000 words done\n",
      "5000 words done\n",
      "6000 words done\n",
      "7000 words done\n",
      "8000 words done\n",
      "9000 words done\n",
      "10000 words done\n",
      "11000 words done\n",
      "12000 words done\n",
      "13000 words done\n",
      "14000 words done\n",
      "15000 words done\n"
     ]
    }
   ],
   "source": [
    "filtered2 = save_dict([stats2,stats3,stats4], 'dict_2-4gram_10000-f.yml', filtered_pos=filtered)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive stats:\n",
      "\tJJ   :      6.473  | =the _ JJ NN          :  94603  50\n",
      "\tVB   :      6.440  | =to _                 :  505933  57\n",
      "\tNN   :      6.124  | =a JJ _               :  366325  20\n",
      "\tNNP  :      4.468  | _ NNP                 :  4868162  131\n",
      "\tVBP  :      2.102  | NNS _                 :  775541  41\n",
      "\tSOL  :      1.368  | _ RB                  :  969914  37\n",
      "\tEQN  :      1.262  | =and _                :  795909  52\n",
      "\tVBZ  :      0.731  | _ RB                  :  969914  37\n",
      "\tVBN  :      0.718  | RB _                  :  774420  48\n",
      "\tNNS  :      0.637  | NN =of _              :  571061  64\n",
      "\tRB   :      0.529  | _ JJ                  :  1807637  166\n",
      "\tCD   :      0.305  | , _                   :  1737141  37\n",
      "\tMD   :      0.231  | _ RB                  :  969914  37\n",
      "\tVBG  :      0.205  | _ JJ NNS              :  278857  24\n",
      "\tVBD  :      0.140  | NNS _                 :  775541  41\n",
      "\tFW   :      0.107  | NNP _ NN              :  432452  23\n",
      "\tRBS  :      0.096  | =the _ JJ             :  164120  64\n",
      "\tJJR  :      0.080  | RB _                  :  774420  48\n",
      "\tRBR  :      0.074  | _ JJ                  :  1807637  166\n",
      "\tEQNP :      0.073  | _ EQNP                :  47587  20\n",
      "\tJJS  :      0.055  | =the _ JJ             :  164120  64\n",
      "\tPOS  :      0.031  | NNP _ NN              :  432452  23\n",
      "\tUH   :      0.017  | _ ,                   :  2747045  37\n",
      "\tRP   :      0.007  | _ JJ NNS              :  278857  24\n",
      "\tNNPS :      0.006  | =of _                 :  869994  92\n",
      "\tSYM  :      0.002  | _ NN (                :  158557  20\n",
      "\tLS   :      0.000  | _ ,                   :  2747045  37\n",
      "\tFOOTNOTE:      0.000  | JJ _ NN               :  624286  72\n",
      "Negative stats:\n",
      "\tUH   : 5.53813e-07  | JJ _                  :  3661301  121\n",
      "\tLS   : 7.58695e-07  | NNP _                 :  4791145  45\n",
      "\tFOOTNOTE: 1.66144e-06  | JJ _                  :  3661301  121\n",
      "\tSYM  : 2.64171e-06  | =a _                  :  800272  73\n",
      "\tVBZ  : 3.24426e-06  | =the _ NN             :  1135652  170\n",
      "\tVBP  : 3.24426e-06  | =the _ NN             :  1135652  170\n",
      "\tJJS  : 3.46028e-06  | MD _                  :  347744  31\n",
      "\tPOS  : 3.60221e-06  | =a JJ _               :  366325  20\n",
      "\tVB   : 3.60221e-06  | =a _ NN               :  388084  51\n",
      "\tRP   : 3.60221e-06  | =a _ NN               :  388084  51\n",
      "\tRBS  : 6.54939e-06  | =in _                 :  481384  22\n",
      "\tVBD  : 7.20443e-06  | =a _ NN               :  388084  51\n",
      "\tNNPS : 7.20443e-06  | =a _ NN               :  388084  51\n",
      "\tMD   : 7.21548e-06  | JJ _ NN               :  624286  72\n",
      "\tRBR  : 1.01989e-05  | =the _ NN NN          :  164780  58\n",
      "\tJJR  : 1.38411e-05  | MD _                  :  347744  31\n",
      "\tFW   : 1.55335e-05  | =in =the _ NN         :  107900  20\n",
      "\tEQNP : 3.46028e-05  | MD _                  :  347744  31\n",
      "\tVBG  : 5.88248e-05  | MD _                  :  347744  31\n",
      "\tNNS  : 0.000221458  | MD _                  :  347744  31\n",
      "\tJJ   : 0.000328727  | MD _                  :  347744  31\n",
      "\tRB   : 0.000342106  | =of =the _ NN         :  171784  42\n",
      "\tCD   : 0.000429075  | MD _                  :  347744  31\n",
      "\tVBN  : 0.000432535  | MD _                  :  347744  31\n",
      "\tNNP  : 0.00120072  | MD _                  :  347744  31\n",
      "\tNN   : 0.00211423  | MD _                  :  347744  31\n",
      "\tEQN  : 0.00234434  | RBR _                 :  37791  35\n",
      "\tSOL  :  0.0162587  | _ ,                   :  2747045  37\n"
     ]
    }
   ],
   "source": [
    "word = 'diffuse'\n",
    "pos2, rej2 = stats2.calc_specific(word, accept_threshold=(100, 20), reject_threshold=(100, 5))\n",
    "pos3, rej3 = stats3.calc_specific(word, accept_threshold=(100, 20), reject_threshold=(100, 5))\n",
    "pos4, rej4 = stats4.calc_specific(word, accept_threshold=(100, 20), reject_threshold=(100, 5))\n",
    "join_pos_rej([pos2,pos3,pos4], [rej2,rej3,rej4])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "commute:\n",
      "NN: 0.152\n",
      "NNS: 0.095\n",
      "RB: 0.083\n",
      "JJ: 0.083\n",
      "VBN: 0.065\n",
      "NNP: 0.064\n",
      "VBP: 0.053\n",
      "VB: 0.049\n",
      "VBZ: 0.049\n",
      "CD: 0.038\n",
      "VBG: 0.036\n",
      "VBD: 0.030\n",
      "MD: 0.023\n",
      "JJR: 0.019\n",
      "RBR: 0.018\n",
      "FW: 0.017\n",
      "RP: 0.016\n",
      "JJS: 0.016\n",
      "POS: 0.016\n",
      "SYM: 0.016\n",
      "NNPS: 0.015\n",
      "RBS: 0.015\n",
      "UH: 0.015\n",
      "LS: 0.015\n",
      "=============================================\n",
      "denote:\n",
      "VB: 0.138\n",
      "VBP: 0.119\n",
      "VBZ: 0.073\n",
      "NN: 0.072\n",
      "RB: 0.063\n",
      "VBG: 0.058\n",
      "NNP: 0.054\n",
      "JJ: 0.054\n",
      "VBD: 0.046\n",
      "NNS: 0.040\n",
      "VBN: 0.039\n",
      "CD: 0.032\n",
      "MD: 0.023\n",
      "JJR: 0.019\n",
      "FW: 0.018\n",
      "JJS: 0.018\n",
      "RP: 0.018\n",
      "RBR: 0.018\n",
      "RBS: 0.017\n",
      "POS: 0.017\n",
      "NNPS: 0.016\n",
      "SYM: 0.016\n",
      "UH: 0.016\n",
      "LS: 0.016\n",
      "=============================================\n",
      "recall:\n",
      "VB: 0.124\n",
      "VBP: 0.109\n",
      "NN: 0.098\n",
      "JJ: 0.069\n",
      "RB: 0.065\n",
      "NNP: 0.063\n",
      "VBG: 0.054\n",
      "VBZ: 0.051\n",
      "VBD: 0.047\n",
      "VBN: 0.042\n",
      "NNS: 0.038\n",
      "CD: 0.032\n",
      "MD: 0.021\n",
      "JJR: 0.019\n",
      "RP: 0.019\n",
      "RBR: 0.018\n",
      "JJS: 0.018\n",
      "FW: 0.017\n",
      "UH: 0.016\n",
      "POS: 0.016\n",
      "RBS: 0.016\n",
      "NNPS: 0.016\n",
      "SYM: 0.016\n",
      "LS: 0.016\n",
      "=============================================\n",
      "decrease:\n",
      "NN: 0.359\n",
      "NNS: 0.114\n",
      "JJ: 0.072\n",
      "NNP: 0.067\n",
      "RB: 0.043\n",
      "VB: 0.042\n",
      "VBN: 0.034\n",
      "VBZ: 0.033\n",
      "VBG: 0.029\n",
      "CD: 0.029\n",
      "VBP: 0.025\n",
      "VBD: 0.020\n",
      "JJR: 0.015\n",
      "MD: 0.015\n",
      "JJS: 0.013\n",
      "RBR: 0.013\n",
      "RP: 0.012\n",
      "FW: 0.011\n",
      "RBS: 0.011\n",
      "NNPS: 0.011\n",
      "POS: 0.011\n",
      "SYM: 0.011\n",
      "UH: 0.011\n",
      "=============================================\n",
      "increases:\n",
      "NN: 0.172\n",
      "VBZ: 0.120\n",
      "NNS: 0.118\n",
      "RB: 0.064\n",
      "NNP: 0.061\n",
      "JJ: 0.056\n",
      "VBN: 0.048\n",
      "VBP: 0.044\n",
      "VBG: 0.040\n",
      "VBD: 0.036\n",
      "CD: 0.029\n",
      "VB: 0.027\n",
      "MD: 0.022\n",
      "JJR: 0.015\n",
      "RBR: 0.015\n",
      "FW: 0.014\n",
      "RP: 0.014\n",
      "JJS: 0.014\n",
      "NNPS: 0.013\n",
      "POS: 0.013\n",
      "RBS: 0.013\n",
      "SYM: 0.013\n",
      "UH: 0.013\n",
      "LS: 0.013\n",
      "FOOTNOTE: 0.013\n",
      "=============================================\n"
     ]
    }
   ],
   "source": [
    "for w in ['commute','denote','recall','decrease','increases']:\n",
    "    stats.print_stats(w)\n",
    "    #stats3.print_stats(w)\n",
    "    #stats4.print_stats(w)\n",
    "    print(\"=============================================\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "texts = load_tag_texts(\"/Users/Gleb/Desktop/Solver/2020-09-08-arxiv-extracts-nofallback-until-2007-068.tar\", max_num=10000)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [],
   "source": [
    "# dump texts to file\n",
    "with open('texts.pickle', 'wb') as f:\n",
    "    pickle.dump(texts, f)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "outputs": [],
   "source": [
    "# load texts from file\n",
    "with open('texts.pickle', 'rb') as f:\n",
    "    texts = pickle.load(f)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "data": {
      "text/plain": "2467700"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gc\n",
    "\n",
    "texts = list(texts[:1000])\n",
    "gc.collect()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: unknown environment equation\n",
      "Warning: unknown environment equation\n",
      "Warning: unknown environment equation\n",
      "Warning: unknown environment equation\n",
      "Warning: unknown environment equation\n",
      "Warning: unknown environment restatable\n",
      "Warning: unknown environment equation\n",
      "Warning: unknown environment restatable\n",
      "Warning: unknown environment equation*\n",
      "Warning: unknown environment multline*\n",
      "Warning: unknown environment align*\n",
      "Warning: unknown environment restatable\n",
      "Warning: unknown environment equation*\n",
      "Warning: unknown environment equation\n",
      "Warning: unknown environment multline\n",
      "Warning: unknown environment equation\n",
      "Warning: unknown environment align*\n",
      "Warning: unknown environment cases\n",
      "Warning: unknown environment equation\n",
      "Warning: unknown environment cases\n",
      "Warning: unknown environment equation*\n",
      "226 sentences; collecting stats for 1-grams\n",
      "collecting stats for words\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "stats = collect_ngram_stats(1, 'tests/main.tex')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "for key, value in sorted(stats.ngrams_by_word['commute'].items(), key=lambda x: x[1], reverse=True):\n",
    "    print(f'{\" \".join(str(x or \"_\") for x in key)}: {value}')\n",
    "    for ps, num in sorted(stats.ngrams_by_pos[key].items(), key=lambda x: x[1], reverse=True)[:3]:\n",
    "        print(f'\\t{ps}: {num}')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "data": {
      "text/plain": "136524"
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(1 for w in stats.words if w and w[0].isalpha())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "Can't pickle local object 'NgramStats.collect_stats_for_words.<locals>.<lambda>'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mAttributeError\u001B[0m                            Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-16-1170021de673>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[0;32mwith\u001B[0m \u001B[0mopen\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m'word_stats_2.dump'\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m'wb'\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;32mas\u001B[0m \u001B[0mf\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 2\u001B[0;31m     \u001B[0mpickle\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mdump\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mstats\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mf\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m",
      "\u001B[0;31mAttributeError\u001B[0m: Can't pickle local object 'NgramStats.collect_stats_for_words.<locals>.<lambda>'"
     ]
    }
   ],
   "source": [
    "with open('word_stats_2.dump', 'wb') as f:\n",
    "    pickle.dump(stats, f)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "data": {
      "text/plain": "{'#',\n '$',\n \"''\",\n '(',\n ')',\n ',',\n '.',\n ':',\n 'CC',\n 'CD',\n 'DT',\n 'EG',\n 'EQN',\n 'EQNP',\n 'EX',\n 'FOOTNOTE',\n 'FW',\n 'IE',\n 'IN',\n 'JJ',\n 'JJR',\n 'JJS',\n 'LS',\n 'MD',\n 'NN',\n 'NNP',\n 'NNPS',\n 'NNS',\n 'PDT',\n 'POS',\n 'PRP',\n 'PRP$',\n 'RB',\n 'RBR',\n 'RBS',\n 'RP',\n 'SOL',\n 'SYM',\n 'TO',\n 'UH',\n 'VB',\n 'VBD',\n 'VBG',\n 'VBN',\n 'VBP',\n 'VBZ',\n 'WDT',\n 'WP',\n 'WP$',\n 'WRB',\n '``'}"
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(sum(map(list, stats.ngrams.keys()), []))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (input): Linear(in_features=520, out_features=100, bias=True)\n",
      "  (fc0): Linear(in_features=100, out_features=50, bias=True)\n",
      "  (relu0): ReLU()\n",
      "  (dropout0): Dropout(p=0.5, inplace=False)\n",
      "  (fc_out): Linear(in_features=50, out_features=21, bias=True)\n",
      "  (relu_out): ReLU()\n",
      "  (softmax): Softmax(dim=1)\n",
      ")\n",
      "9196 samples\n",
      "epoch   10: loss  0.0237;\t test loss  0.0248\n",
      "epoch   20: loss  0.0196;\t test loss  0.0208\n",
      "epoch   30: loss  0.0177;\t test loss  0.0187\n",
      "epoch   40: loss  0.0168;\t test loss  0.0182\n",
      "epoch   50: loss  0.0163;\t test loss  0.0180\n",
      "epoch   60: loss  0.0160;\t test loss  0.0180\n",
      "epoch   70: loss  0.0156;\t test loss  0.0181\n",
      "epoch   80: loss  0.0152;\t test loss  0.0181\n",
      "epoch   90: loss  0.0150;\t test loss  0.0183\n",
      "epoch  100: loss  0.0146;\t test loss  0.0186\n",
      "epoch  110: loss  0.0144;\t test loss  0.0186\n",
      "epoch  120: loss  0.0142;\t test loss  0.0186\n",
      "epoch  130: loss  0.0140;\t test loss  0.0188\n",
      "epoch  140: loss  0.0138;\t test loss  0.0188\n",
      "epoch  150: loss  0.0137;\t test loss  0.0190\n",
      "early stopping\n",
      "loss 0.01805570349097252\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "#neural network predicting part of speech for a given word (input is a word)\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "\n",
    "# output is probabilities of tags: 'FW', 'IE', 'JJ', 'JJR', 'JJS', 'MD', 'NN', 'NNP', 'NNPS', 'NNS', 'POS',\n",
    "# 'RB', 'RBR', 'RBS', 'RP', 'VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ'\n",
    "# network consists of k fully connected layers, each with n neurons, last layer has a neuron for each tag\n",
    "# input is a word, output is a vector of probabilities for each tag\n",
    "pos_list = ['FW', 'IE', 'JJ', 'JJR', 'JJS', 'MD', 'NN', 'NNP', 'NNPS', 'NNS', 'POS', 'RB', 'RBR', 'RBS', 'RP', 'VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ']\n",
    "class PosNet:\n",
    "    def __init__(self, n: List[int], max_word_len: int = 20):\n",
    "        self.k = len(n)-1\n",
    "        self.n = n\n",
    "        self.max_word_len = max_word_len\n",
    "        self.net = nn.Sequential()\n",
    "        # each letter of input word is mapped to a vector of length 26 (26 letters in alphabet)\n",
    "        self.net.add_module('input', nn.Linear(max_word_len*26, n[0]))\n",
    "        for i in range(self.k):\n",
    "            self.net.add_module(f'fc{i}', nn.Linear(n[i], n[i+1]))\n",
    "            self.net.add_module(f'relu{i}', nn.ReLU())\n",
    "            self.net.add_module(f'dropout{i}', nn.Dropout(0.5))\n",
    "\n",
    "        self.net.add_module('fc_out', nn.Linear(n[-1], len(pos_list)))\n",
    "        self.net.add_module('relu_out', nn.ReLU())\n",
    "        #self.net.add_module('sigmoid', nn.Sigmoid())\n",
    "        self.net.add_module('softmax', nn.Softmax(dim=1))\n",
    "        print(self.net)\n",
    "\n",
    "\n",
    "    def train(self, data, epochs=10, test_data=None):\n",
    "        self.net.train()\n",
    "        optimizer = optim.Adam(self.net.parameters(), lr=0.01)\n",
    "\n",
    "        # convert data to vector of size 27*max_word_len (word is aligned to the end)\n",
    "        input = torch.tensor([self.get_letter_vector(w) for w, _ in data], dtype=torch.float32)\n",
    "        # convert tags to vector of size len(pos_list)\n",
    "        target = torch.tensor([self.get_tag_vector(t) for _, t in data], dtype=torch.float32)\n",
    "        positive_idx = [i for i, t in enumerate(target) if sum(t) > 0]\n",
    "        input = input[positive_idx]\n",
    "        target = target[positive_idx]\n",
    "        if test_data:\n",
    "            test_input = torch.tensor([self.get_letter_vector(w) for w, _ in test_data], dtype=torch.float32)\n",
    "            test_target = torch.tensor([self.get_tag_vector(t) for _, t in test_data], dtype=torch.float32)\n",
    "            test_positive_idx = [i for i, t in enumerate(test_target) if sum(t) > 0]\n",
    "            test_input = test_input[test_positive_idx]\n",
    "            test_target = test_target[test_positive_idx]\n",
    "        else:\n",
    "            test_input = None\n",
    "            test_target = None\n",
    "\n",
    "        print(f'{len(input)} samples')\n",
    "        test_res = []\n",
    "        min_test = 1e10\n",
    "        for epoch in range(epochs):\n",
    "            # train\n",
    "            optimizer.zero_grad()\n",
    "            output = self.net(input)\n",
    "            loss = F.mse_loss(output, target)\n",
    "            #loss /= len(data)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            if test_data:\n",
    "                test_output = self.net(test_input)\n",
    "                test_loss = F.mse_loss(test_output, test_target)\n",
    "                test_res.append(test_loss.item())\n",
    "            # print loss and check if it is converged\n",
    "            if epoch % 10 == 9:\n",
    "                if test_data:\n",
    "                    mean_test = np.mean(test_res[-10:])\n",
    "                    print(f'epoch {epoch+1:4}: loss {loss.item():7.4f};\\t test loss {mean_test:7.4f}')\n",
    "                    if len(test_res)>=20 and mean_test > min_test+0.001:\n",
    "                        print('early stopping')\n",
    "                        break\n",
    "                    min_test = min(min_test, mean_test)\n",
    "                else:\n",
    "                    print(f'epoch {epoch+1}: loss {loss.item():.4f}')\n",
    "\n",
    "        self.net.eval()\n",
    "        if test_data:\n",
    "            return test_res[-1]\n",
    "\n",
    "    def validate(self, data):\n",
    "        self.net.eval()\n",
    "        input = torch.tensor([self.get_letter_vector(w) for w, _ in data], dtype=torch.float32)\n",
    "        target = torch.tensor([self.get_tag_vector(t) for _, t in data], dtype=torch.float32)\n",
    "        positive_idx = [i for i, t in enumerate(target) if sum(t) > 0]\n",
    "        input = input[positive_idx]\n",
    "        target = target[positive_idx]\n",
    "        output = self.net(input)\n",
    "        loss = F.mse_loss(output, target)\n",
    "        print(f'loss {loss}')\n",
    "        return loss\n",
    "\n",
    "    def test(self, word, max_items=None):\n",
    "        if max_items is None:\n",
    "            max_items = len(pos_list)\n",
    "        self.net.eval()\n",
    "        input = torch.tensor([self.get_letter_vector(word)], dtype=torch.float32)\n",
    "        output = self.net(input)[0]\n",
    "        #print parts of speech for each word with highest probabilities\n",
    "        res = sorted(zip(pos_list, output), key=lambda x: x[1], reverse=True)\n",
    "        for i in range(min(max_items, len(res))):\n",
    "            print(f'{i+1:3}. {res[i][0]:5} {res[i][1]:.4f}') # pos, probability\n",
    "        return res\n",
    "\n",
    "    def get_letter_vector(self, word):\n",
    "        res = [0]*26*self.max_word_len\n",
    "        start = (self.max_word_len - len(word))*26\n",
    "        for i, c in enumerate(word):\n",
    "            res[start+i*26+ord(c)-ord('a')] = 1\n",
    "        return res\n",
    "\n",
    "    def get_tag_vector(self, tags: Dict[str, float], threshold=5):\n",
    "        res = torch.zeros(len(pos_list), dtype=torch.float32)\n",
    "        for tag, prob in tags.items():\n",
    "            if tag in pos_list and prob > threshold:\n",
    "                res[pos_list.index(tag)] = 1\n",
    "        if res.sum() > 0:\n",
    "            res /= res.sum()\n",
    "        return list(res)\n",
    "\n",
    "    # redurns a list of tags with their probabilities in decreasing order\n",
    "    def predict(self, word):\n",
    "        self.net.eval()\n",
    "        input = self.get_letter_vector(word)\n",
    "        output = self.net(input)\n",
    "        return [(pos_list[i], output[0, i].item()) for i in range(len(pos_list))]\n",
    "\n",
    "train_data = [(k,v) for k,v in filtered.items() if len(k) <= 20 and k.isalpha() and all(ord('a')<=ord(c)<=ord('z') for c in k)]\n",
    "# shuffle train_data\n",
    "random.shuffle(train_data)\n",
    "test_data = train_data[:1000]\n",
    "train_data = train_data[1000:]\n",
    "\n",
    "# train the network\n",
    "net = PosNet(n=[100,50], max_word_len=20)\n",
    "net.train(train_data, epochs=1000, test_data=test_data)\n",
    "\n",
    "# test the network\n",
    "test_loss = net.validate(test_data)\n",
    "#print(f'test loss: {test_loss}')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "outputs": [
    {
     "data": {
      "text/plain": "array([[0.01450993, 0.01450993, 0.0496132 , 0.01450993, 0.01450993,\n        0.01450993, 0.08943307, 0.01450993, 0.01450993, 0.01450993,\n        0.01450993, 0.43111312, 0.01450993, 0.01450993, 0.01450993,\n        0.1482599 , 0.01450993, 0.01450993, 0.026211  , 0.03772076,\n        0.01450993]], dtype=float32)"
     },
     "execution_count": 230,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#net = PosNet(k=2, n=100, max_word_len=20)\n",
    "net.net.eval()\n",
    "net.net(torch.tensor([net.get_letter_vector('tensorly')], dtype=torch.float32)).detach().numpy()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "outputs": [
    {
     "data": {
      "text/plain": "21692284"
     },
     "execution_count": 199,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del stats2\n",
    "del stats3\n",
    "del stats4\n",
    "gc.collect()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "outputs": [
    {
     "data": {
      "text/plain": "OrderedDict([('input.weight',\n              tensor([[ 0.0171, -0.0024, -0.0251,  ...,  0.1406, -0.2134,  0.0371],\n                      [ 0.0186,  0.0428,  0.0091,  ...,  0.0467,  0.0746,  0.1979],\n                      [ 0.0193,  0.0410,  0.0380,  ...,  0.0066,  0.1092, -0.0121],\n                      ...,\n                      [-0.0269,  0.0293, -0.0058,  ...,  0.0713, -0.1289,  0.0541],\n                      [-0.0101,  0.0206, -0.0212,  ..., -0.1208, -0.0140, -0.1371],\n                      [ 0.0188,  0.0199,  0.0094,  ...,  0.0652,  0.0213,  0.2003]])),\n             ('input.bias',\n              tensor([ 0.0643,  0.0928,  0.1025, -0.0282, -0.0124,  0.0623,  0.1137,  0.0053,\n                       0.1082,  0.0482,  0.0822,  0.1060, -0.0467,  0.0660, -0.0989, -0.0980,\n                      -0.1000, -0.0574,  0.0804,  0.1122, -0.0659, -0.0944,  0.0813, -0.1261,\n                      -0.1304,  0.0544, -0.0825, -0.1079,  0.0181, -0.1151, -0.1173,  0.0976,\n                       0.0352, -0.0333, -0.0719, -0.0904, -0.1318,  0.0396,  0.0682, -0.0534,\n                       0.1016,  0.0209, -0.1101,  0.0872, -0.1103, -0.0989, -0.0461, -0.0946,\n                      -0.0472,  0.1478, -0.0885,  0.0082,  0.1077,  0.0775,  0.1919, -0.1119,\n                       0.0734, -0.1217,  0.1453,  0.0960, -0.0232, -0.0421,  0.0218,  0.0887,\n                       0.0884, -0.0550, -0.0020, -0.0944,  0.0577,  0.0590, -0.0688,  0.0110,\n                      -0.1023, -0.0867,  0.1231, -0.1052, -0.0087, -0.1666, -0.1005, -0.0801,\n                      -0.0170,  0.1328,  0.0290,  0.0442,  0.0504, -0.0550,  0.0917,  0.0941,\n                      -0.0887, -0.0951, -0.1239,  0.0866, -0.0438, -0.0267, -0.0891, -0.0816,\n                       0.1388,  0.0575, -0.1237,  0.0409])),\n             ('fc0.weight',\n              tensor([[-0.0630, -0.0397, -0.0398,  ..., -0.0397, -0.0640,  0.0151],\n                      [ 0.2354,  0.0880, -0.2951,  ...,  0.0251,  0.4202,  0.1676],\n                      [ 0.0998,  0.0668, -0.2786,  ...,  0.1123, -0.0261,  0.0861],\n                      ...,\n                      [-0.2029,  0.2842,  0.0916,  ...,  0.0550,  0.1246, -0.0152],\n                      [ 0.1113,  0.0966, -0.0933,  ...,  0.1152, -0.0284, -0.0896],\n                      [ 0.1018, -0.0203, -0.2172,  ...,  0.1790, -0.1491,  0.0151]])),\n             ('fc0.bias',\n              tensor([-0.0558, -0.2557,  0.0059,  0.0098,  0.0807, -0.1725,  0.0215, -0.0197,\n                      -0.0444,  0.0339,  0.1549,  0.1401,  0.0143,  0.0753, -0.0372,  0.0764,\n                       0.0675,  0.0189,  0.0831,  0.0860, -0.0628,  0.0378,  0.1411,  0.0661,\n                       0.0278,  0.0246,  0.0986,  0.0550,  0.0244,  0.0986, -0.0483,  0.0512,\n                       0.0028,  0.1263, -0.1979,  0.0235,  0.0302,  0.0756, -0.0311,  0.0628,\n                       0.0756,  0.1389, -0.0439,  0.0686,  0.0788, -0.0303, -0.0649, -0.0205,\n                       0.0776, -0.0147])),\n             ('fc_out.weight',\n              tensor([[ 0.0155, -0.0825, -0.0067,  ..., -0.0486, -0.0045, -0.0629],\n                      [-0.0142, -0.0611, -0.1248,  ..., -0.0107,  0.0670,  0.0400],\n                      [ 0.0349, -0.2873,  0.1949,  ..., -0.5280,  0.0246, -0.0964],\n                      ...,\n                      [-0.0809,  0.0755,  0.2410,  ...,  0.1989,  0.1357,  0.1378],\n                      [ 0.0473,  0.0464,  0.0170,  ..., -0.0472, -0.1485, -0.1340],\n                      [ 0.1052,  0.0418,  0.0280,  ..., -0.1312,  0.0032, -0.0731]])),\n             ('fc_out.bias',\n              tensor([-0.1303,  0.0735,  0.1433,  0.0462, -0.2341,  0.0560,  0.1154, -0.0684,\n                      -0.0723,  0.0586, -0.2057, -0.1355,  0.0847, -0.2614,  0.0911,  0.1392,\n                       0.0505, -0.0235,  0.0345,  0.0710,  0.1192]))])"
     },
     "execution_count": 258,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.net.state_dict()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "string keys in translate table must be of length 1",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-325-2873624b1280>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[0;32m----> 1\u001B[0;31m \u001B[0;34m'abcd'\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mtranslate\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mstr\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mmaketrans\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m{\u001B[0m\u001B[0;34m'ab'\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m'2'\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m'c'\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m'5'\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m'd'\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m'a'\u001B[0m\u001B[0;34m}\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m",
      "\u001B[0;31mValueError\u001B[0m: string keys in translate table must be of length 1"
     ]
    }
   ],
   "source": [
    "'abcd'.translate(str.maketrans({'ab':'2','c':'5','d':'a'}))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "One TwO One TwO One\n"
     ]
    }
   ],
   "source": [
    "s = 'one two one two one'\n",
    "\n",
    "print(s.translate(str.maketrans({'o': 'O', 't': 'T'})))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named '__mp_main__'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mModuleNotFoundError\u001B[0m                       Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-3-e7c984499a7c>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m     11\u001B[0m     \u001B[0;32mreturn\u001B[0m \u001B[0mtrees\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     12\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 13\u001B[0;31m \u001B[0mtrees\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mread_trees\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m'results'\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m",
      "\u001B[0;32m<ipython-input-3-e7c984499a7c>\u001B[0m in \u001B[0;36mread_trees\u001B[0;34m(dir)\u001B[0m\n\u001B[1;32m      8\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0mf\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mendswith\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m'.pickle'\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      9\u001B[0m             \u001B[0;32mwith\u001B[0m \u001B[0mopen\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mos\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mpath\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mjoin\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mdir\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mf\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m'rb'\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;32mas\u001B[0m \u001B[0mf\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 10\u001B[0;31m                 \u001B[0mtrees\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mappend\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mpickle\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mload\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mf\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     11\u001B[0m     \u001B[0;32mreturn\u001B[0m \u001B[0mtrees\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     12\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mModuleNotFoundError\u001B[0m: No module named '__mp_main__'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pickle\n",
    "# read parse trees from files from directory results/*.pickle\n",
    "\n",
    "def read_trees(dir):\n",
    "    trees = []\n",
    "    for f in os.listdir(dir):\n",
    "        if f.endswith('.pickle'):\n",
    "            with open(os.path.join(dir, f), 'rb') as f:\n",
    "                trees.append(pickle.load(f))\n",
    "    return trees\n",
    "\n",
    "trees = read_trees('results')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "import pickle\n",
    "mobypos_dict = {}\n",
    "mobypos_comb_dict = {}\n",
    "with open('dicts/mobypos.txt', 'r', encoding='ascii',errors='ignore') as f:\n",
    "    for line in f:\n",
    "        word, tag = line.split('\\\\')\n",
    "        if ' ' in word:\n",
    "            mobypos_comb_dict[word] = tag\n",
    "        else:\n",
    "            mobypos_dict[word] = tag\n",
    "\n",
    "# dump the dictionaries to pickle file\n",
    "with open('dicts/mobypos.pickle', 'wb') as f:\n",
    "    pickle.dump((mobypos_dict, mobypos_comb_dict), f)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import tagger\n",
    "dicts = tagger.Dictionaries()\n",
    "dicts.load_from_src('dicts')\n",
    "dicts.save_json_compressed('dicts/dicts.gz')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "data": {
      "text/plain": "35939"
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(mobypos_comb_dict)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}